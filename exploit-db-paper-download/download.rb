#!/usr/bin/ruby

require 'nokogiri'
require 'open-uri'
require 'net/https'
require 'zlib'

#######
#
# downloads the papers as pdfs from www.exploit-db.com and stores them in the directory "pdfs" 
#
#
#######
base_uri="https://www.exploit-db.com/papers/?order_by=date_published&order=desc&pg="
rel_down_dir = "pdfs"



page_ctr = 1
newFilesForDownloadAvailable = true
downloadMap = {}

while newFilesForDownloadAvailable do
   doc = Nokogiri::HTML(open(base_uri + page_ctr.to_s, 'User-Agent' => 'firefox'))
   page_ctr += 1

   hrefs = doc.css("a").map do |link|
      if (href = link.attr("href")) && !href.empty? && href.end_with?(".pdf")
         file = href.split('/')[-1]

         if downloadMap.has_key?(file)
            # already handled
         elsif File.file?(rel_down_dir + "/" + file)
            print("file " + file + " exists already\n")
            newFilesForDownloadAvailable = false
         else
            File.open(rel_down_dir + "/" + file, "wb") do |f|
               IO.copy_stream(open(href, 'User-Agent' => 'firefox'), f)
            end
            downloadMap[file]=nil
            print(href + "\n")
         end
      end
   end.compact.uniq


   print "\nPage " + page_ctr.to_s + "\n\n"
end
